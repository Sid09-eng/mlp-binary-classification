{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeb7f59d-d0a4-4de5-af6f-9e134609bc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (301, 34)\n",
      "Test data shape: (50, 33)\n",
      "MLP(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=33, out_features=20, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=10, out_features=1, bias=True)\n",
      "    (5): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Epoch 10/100, Train Loss: 0.4181, Val Loss: 0.4791, Val Accuracy: 0.8197\n",
      "Epoch 20/100, Train Loss: 0.2064, Val Loss: 0.3432, Val Accuracy: 0.8689\n",
      "Epoch 30/100, Train Loss: 0.1139, Val Loss: 0.3332, Val Accuracy: 0.8525\n",
      "Epoch 40/100, Train Loss: 0.0699, Val Loss: 0.3664, Val Accuracy: 0.8197\n",
      "Epoch 50/100, Train Loss: 0.0441, Val Loss: 0.4290, Val Accuracy: 0.8033\n",
      "Epoch 60/100, Train Loss: 0.0294, Val Loss: 0.4761, Val Accuracy: 0.8197\n",
      "Epoch 70/100, Train Loss: 0.0208, Val Loss: 0.5166, Val Accuracy: 0.8197\n",
      "Epoch 80/100, Train Loss: 0.0154, Val Loss: 0.5538, Val Accuracy: 0.8197\n",
      "Epoch 90/100, Train Loss: 0.0119, Val Loss: 0.5844, Val Accuracy: 0.8197\n",
      "Epoch 100/100, Train Loss: 0.0095, Val Loss: 0.6121, Val Accuracy: 0.8197\n",
      "Predictions for test data:\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1.]\n",
      "Total samples: 50\n",
      "Class 0: 1\n",
      "Class 1: 49\n",
      "\n",
      "Model Architecture:\n",
      "Input features: 33\n",
      "Hidden layers: [20, 10]\n",
      "Output: 1\n",
      "Activation function: Tanh (hidden layers), Sigmoid (output layer)\n",
      "Loss function: Binary Cross Entropy\n",
      "Optimizer: Adam (learning rate = 0.001)\n",
      "Best validation accuracy: 0.8852\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Load the data\n",
    "train_data = pd.read_csv('data_train.csv', header=None)\n",
    "test_data = pd.read_csv('data_test.csv', header=None)\n",
    "\n",
    "# Check the shapes\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "# For training data, assume first column is the label\n",
    "X_train_full = train_data.iloc[:, 1:].values  # Features (all columns except first)\n",
    "y_train_full = train_data.iloc[:, 0].values   # Labels (first column)\n",
    "\n",
    "# For test data, all columns are features\n",
    "X_test = test_data.values\n",
    "\n",
    "# 2. Preprocess the data\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_full = scaler.fit_transform(X_train_full)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "y_val_tensor = torch.FloatTensor(y_val).unsqueeze(1)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 3. Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # Create hidden layers with tanh activation\n",
    "        for h_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, h_size))\n",
    "            layers.append(nn.Tanh())\n",
    "            prev_size = h_size\n",
    "        \n",
    "        # Output layer with sigmoid for binary classification\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = X_train.shape[1]  # Number of features\n",
    "hidden_sizes = [20, 10]  # Two hidden layers with 20 and 10 neurons\n",
    "output_size = 1  # Binary classification\n",
    "\n",
    "# Initialize the model\n",
    "model = MLP(input_size, hidden_sizes, output_size)\n",
    "print(model)\n",
    "\n",
    "# 4. Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 5. Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                predicted = (outputs >= 0.5).float()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_val_loss = running_loss / len(val_loader.dataset)\n",
    "        epoch_val_acc = correct / total\n",
    "        \n",
    "        val_losses.append(epoch_val_loss)\n",
    "        val_accuracies.append(epoch_val_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "                  f'Train Loss: {epoch_train_loss:.4f}, '\n",
    "                  f'Val Loss: {epoch_val_loss:.4f}, '\n",
    "                  f'Val Accuracy: {epoch_val_acc:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses, val_accuracies\n",
    "\n",
    "# 6. Train the model\n",
    "train_losses, val_losses, val_accuracies = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=100\n",
    ")\n",
    "\n",
    "# 7. Plot training and validation metrics\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot losses\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_plots.png')\n",
    "plt.close()\n",
    "\n",
    "# 8. Make predictions on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor)\n",
    "    predicted_classes = (test_predictions >= 0.5).float().squeeze().numpy()\n",
    "\n",
    "# 9. Save test predictions to a file\n",
    "test_predictions_df = pd.DataFrame(predicted_classes)\n",
    "test_predictions_df.to_csv('test_predictions.csv', index=False, header=False)\n",
    "\n",
    "print(\"Predictions for test data:\")\n",
    "print(predicted_classes)\n",
    "print(f\"Total samples: {len(predicted_classes)}\")\n",
    "print(f\"Class 0: {np.sum(predicted_classes == 0)}\")\n",
    "print(f\"Class 1: {np.sum(predicted_classes == 1)}\")\n",
    "\n",
    "# 10. Model Architecture Summary\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(f\"Input features: {input_size}\")\n",
    "print(f\"Hidden layers: {hidden_sizes}\")\n",
    "print(f\"Output: {output_size}\")\n",
    "print(f\"Activation function: Tanh (hidden layers), Sigmoid (output layer)\")\n",
    "print(f\"Loss function: Binary Cross Entropy\")\n",
    "print(f\"Optimizer: Adam (learning rate = 0.001)\")\n",
    "print(f\"Best validation accuracy: {max(val_accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af01e4b-6a57-45a3-87fa-86b52a240524",
   "metadata": {},
   "source": [
    "Explanation:-\n",
    "1. Data Preparation\n",
    "First, I loaded the training data (data_train.csv) and test data (data_test.csv). In the\n",
    "training dataset, the first column represents the class labels, and the remaining 33 columns are\n",
    "features. The test dataset only contains the 33 features and no labels.\n",
    "To ensure the model could train effectively, I standardized all the feature values\n",
    "using StandardScaler, which scales the data to have zero mean and unit variance. I then\n",
    "split the training data into training and validation sets, using an 80/20 split.\n",
    "\n",
    "2. Building the MLP Model\n",
    "Next, I created a feedforward neural network with the following architecture:\n",
    "• Input Layer: 33 neurons (one for each feature)\n",
    "• Two Hidden Layers:\n",
    "o First hidden layer with 20 neurons\n",
    "o Second hidden layer with 10 neurons\n",
    "o Both hidden layers use the tanh activation function\n",
    "• Output Layer: 1 neuron with a sigmoid activation function for binary classification\n",
    "I used PyTorch’s nn.Sequential to stack the layers and activations together in a clean and\n",
    "modular way.\n",
    "\n",
    "3. Training the Model\n",
    "For training, I used the Binary Cross Entropy Loss (BCELoss) since it's a binary\n",
    "classification task. I optimized the model using the Adam optimizer with a learning rate of\n",
    "0.001.\n",
    "I trained the model for 100 epochs, using a batch size of 32. After each epoch, I evaluated the\n",
    "model’s performance on the validation set by calculating both the validation loss and\n",
    "accuracy.\n",
    "To track the training process, I saved plots of the training and validation loss, as well as the\n",
    "validation accuracy over time.\n",
    "\n",
    "4. Making Predictions\n",
    "After training the model, I used it to predict the class labels for the test data. Since the output\n",
    "layer uses a sigmoid activation function, the model outputs a value between 0 and 1. I\n",
    "classified each test sample as:\n",
    "• Class 1 if the output was ≥ 0.5\n",
    "• Class 0 if the output was < 0.5\n",
    "I saved the predicted labels to a file called test_predictions.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93a8cee-7d91-4619-a3eb-47f5740e6619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
